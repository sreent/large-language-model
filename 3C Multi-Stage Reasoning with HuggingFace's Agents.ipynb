{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/large-language-model/blob/main/3C%20Multi-Stage%20Reasoning%20with%20HuggingFace's%20Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBYnnHzI4dhX"
      },
      "source": [
        "# HuggingFace's Agent\n",
        "\n",
        "From Transformers version v4.29 introduces a new API: an API of **tools** and **agents** ğŸ¤©\n",
        "\n",
        "It provides a natural language API on top of transformers: we define a set of curated tools, and design an agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools, but we'll show you how the system can be extended easily to use any tool.\n",
        "\n",
        "Let's start with a few examples of what can be achieved with this new API. It is particularly powerful when it comes to multimodal tasks, so let's take it for a spin to generate images and read text out loud.\n",
        "\n",
        "The accompanying docs are [Transformers Agent](https://huggingface.co/docs/transformers/en/transformers_agents) and [Custom Tools](https://huggingface.co/docs/transformers/en/custom_tools)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5r4UqaQ3jyWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers[torch]==4.32.0\n",
        "!pip -q install huggingface-hub==0.16.4\n",
        "!pip -q install datasets==2.14.4\n",
        "!pip -q install openai==0.27.9\n",
        "!pip -q install diffusers==0.20.0 accelerate==0.21.0 soundfile==0.12.1\n",
        "!pip -q install sentencepiece==0.1.99 opencv-python==4.8.0.76"
      ],
      "metadata": {
        "id": "ZVzqG5318YWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4aa3b10-9a84-4eeb-cd14-866b6a4eb77d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "import soundfile as sf\n",
        "import numpy\n",
        "\n",
        "def play_audio(audio):\n",
        "    sf.write(\"speech_converted.wav\", audio.numpy(), samplerate=16000)\n",
        "    return IPython.display.Audio(\"speech_converted.wav\")"
      ],
      "metadata": {
        "id": "ln7yRA2jlPAE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate API tokens\n",
        "For many of the services that we'll using in the notebook, we'll need some API keys. Follow the instructions below to generate your own.\n",
        "\n",
        "### Hugging Face Hub\n",
        "1. Go to this [Inference API page](https://huggingface.co/inference-api) and click \"Sign Up\" on the top right.\n",
        "\n",
        "<img src=\"https://files.training.databricks.com/images/llm/hf_sign_up.png\" width=700>\n",
        "\n",
        "2. Once you have signed up and confirmed your email address, click on your user icon on the top right and click the `Settings` button.\n",
        "\n",
        "3. Navigate to the `Access Token` tab and copy your token.\n",
        "\n",
        "<img src=\"https://files.training.databricks.com/images/llm/hf_token_page.png\" width=500>"
      ],
      "metadata": {
        "id": "ztneW42xZOiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OPTIONAL: Use OpenAI's language model\n",
        "\n",
        "If you'd rather use OpenAI, you need to generate an OpenAI key.\n",
        "\n",
        "Steps:\n",
        "1. You need to [create an account](https://platform.openai.com/signup) on OpenAI.\n",
        "2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "Note: OpenAI does not have a free option, but it gives you \\$5 as credit. Once you have exhausted your \\$5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing).\n",
        "\n",
        "**IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account!\n"
      ],
      "metadata": {
        "id": "daRife8rStSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Environment File\n",
        "To use LLM models and external services, we need to add access token for HuggingFace and API keys for SerpApi and OpenAI to our environment path.\n",
        "\n",
        "1. Create <code>secret.json</code> and place it in our GDrive.\n",
        "2. Add entries, i.e. key-value pairs in the following format:\n",
        "```{code-block}\n",
        "{\n",
        "    \"HUGGINGFACEHUB_API_TOKEN\": \"<FILL IN>\",\n",
        "    \"OPENAI_API_KEY\": \"<FILL IN>\"\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ny0VwMbIP2Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# path to api keys, i.e. where secrets.json is stored in GDrive\n",
        "SECRET_FILE_PATH = \"/content/drive/MyDrive/secrets.json\"\n",
        "\n",
        "# load environment variables, i.e. tokens or api keys required for HuggingFace, SerpApi and OpenAI API usages\n",
        "with open(SECRET_FILE_PATH, \"r\") as f :\n",
        "    secrets = json.loads(f.read())\n",
        "\n",
        "    for (key, value) in secrets.items() :\n",
        "        os.environ[key] = value"
      ],
      "metadata": {
        "id": "nbMlFyXXZTS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nEHtKNja52V"
      },
      "source": [
        "# Do anything with Transformers\n",
        "\n",
        "We'll start by instantiating an **agent**, which is a large language model (LLM).\n",
        "\n",
        "We recommend using the OpenAI for the best results, but fully open-source models such as StarCoder or OpenAssistant are also available."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Initialization"
      ],
      "metadata": {
        "id": "2rlh1HNVMvGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tools import OpenAiAgent\n",
        "\n",
        "agent = OpenAiAgent(model=\"text-davinci-003\", api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "_4LSImzHMqqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tools import HfAgent\n",
        "\n",
        "#agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")"
      ],
      "metadata": {
        "id": "fsP9aboYN83H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpmYouC4_yzP"
      },
      "source": [
        "## Using the agent\n",
        "\n",
        "The agent is initialized! We now have access to the full power of the tools it has access to.\n",
        "\n",
        "Let's use it ğŸ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJfEhaNTU_nZ"
      },
      "outputs": [],
      "source": [
        "boat = agent.run(\"Generate an image of a boat in the water\")\n",
        "boat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCUV79MbMSwP"
      },
      "source": [
        "If you'd like to hand objects (or previous results!) to the agent, you can do so by passing a variable directly, and mentioning between backticks the name of the variable passed. For example, if I want to re-use the previous boat generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiDsYQJIWcg-"
      },
      "outputs": [],
      "source": [
        "caption = agent.run(\"Can you caption the `boat_image`?\", boat_image=boat)\n",
        "caption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU2WzL02taGg"
      },
      "source": [
        "Agents vary in competency and their capacity to handle several instructions at once; however the strongest of them (such as OpenAI's) are able to handle complex instructions such as the following three-part instruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Az2uYPgsZmW"
      },
      "outputs": [],
      "source": [
        "audio = agent.run(\"Can you generate an image of a boat? Please read out loud the contents of the image afterwards\")\n",
        "play_audio(audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn4396Sst2nV"
      },
      "source": [
        "Where this works great is when your query implies the use of tools which you haven't described directly. An exemple of this would be the following query: \"Read out loud the summary of hf.co\"\n",
        "\n",
        "Here we're asking the model to perform three steps at once:\n",
        "- Fetch the website https://huggingface.co\n",
        "- Summarize it\n",
        "- Translate the text to speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvgWWrVKt24u"
      },
      "outputs": [],
      "source": [
        "audio = agent.run(\"Read out loud the summary of http://hf.co\")\n",
        "play_audio(audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM1IkuDrvdeb"
      },
      "source": [
        "Using the best agents works well ğŸ‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKPC_atPK-JB"
      },
      "source": [
        "### Chat mode\n",
        "\n",
        "So far, we've been using the agent by using it's `.run` command. But that's not the only command it has access to; the second command it has access to is `.chat`, which enables using it in chat mode.\n",
        "\n",
        "The difference between the two is relative to their memory:\n",
        "- `.run` does not keep memory across runs, but performs better for multiple operations at once (such as running two, or three tools in a row from a given instruction)\n",
        "- `.chat` keeps memory across runs, but performs better at single instructions.\n",
        "\n",
        "Let's use it in chat mode!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wns48i10ZdR2"
      },
      "outputs": [],
      "source": [
        "agent.chat(\"Show me an an image of a capybara\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hi_h_xvN1w6"
      },
      "source": [
        "What if we wanted to change something in the image? For example, move the capybaras to a snowy environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHeEmhWxN922"
      },
      "outputs": [],
      "source": [
        "agent.chat(\"Transform the image so that it snows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMGJ-FVgOeDt"
      },
      "source": [
        "Now what if we wanted to remove the capybara in favor of something else? We could ask it to show us a mask of the capybara in the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjhYMIcwOlkV"
      },
      "outputs": [],
      "source": [
        "agent.chat(\"Show me a mask of the snowy capybaras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVYRCGx4x3vH"
      },
      "source": [
        "Having access to the past history is great to repeatedly iterate on a given prompt. However, it has its limitations and sometimes you'd like to have a clean history. In order to do so, you can use the following method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMOjON8CyNJZ"
      },
      "outputs": [],
      "source": [
        "agent.prepare_for_new_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9rx-nKzDpAW"
      },
      "source": [
        "## Tools\n",
        "\n",
        "So far we've been using the tools that the agent has access to. These tools are the following:\n",
        "\n",
        "- **Document question answering**: given a document (such as a PDF) in image format, answer a question on this document (Donut)\n",
        "- **Text question answering**: given a long text and a question, answer the question in the text (Flan-T5)\n",
        "- **Unconditional image captioning**: Caption the image! (BLIP)\n",
        "- **Image question answering**: given an image, answer a question on this image (VILT)\n",
        "- **Image segmentation**: given an image and a prompt, output the segmentation mask of that prompt (CLIPSeg)\n",
        "- **Speech to text**: given an audio recording of a person talking, transcribe the speech into text (Whisper)\n",
        "- **Text to speech**: convert text to speech (SpeechT5)\n",
        "- **Zero-shot text classification**: given a text and a list of labels, identify to which label the text corresponds the most (BART)\n",
        "- **Text summarization**: summarize a long text in one or a few sentences (BART)\n",
        "- **Translation**: translate the text into a given language (NLLB)\n",
        "\n",
        "We also support the following community-based tools:\n",
        "\n",
        "- **Text downloader**: to download a text from a web URL\n",
        "- **Text to image**: generate an image according to a prompt, leveraging stable diffusion\n",
        "- **Image transformation**: transforms an image\n",
        "\n",
        "We can therefore use a mix and match of different tools by explaining in natural language what we would like to do.\n",
        "\n",
        "But what about adding new tools? Let's take a look at how to do that\n",
        "\n",
        "### Adding new tools\n",
        "\n",
        "We'll add a very simple tool so that the demo remains simple: we'll use the awesome cataas (Cat-As-A-Service) API to get random cats on each run.\n",
        "\n",
        "We can get a random cat with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9IOLKJvi2Un"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "image = Image.open(requests.get('https://cataas.com/cat', stream=True).raw)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O3OrmobG0Z2"
      },
      "source": [
        "Let's create a tool that can be used by our system!\n",
        "\n",
        "All tools depend on the superclass Tool that holds the main attributes necessary. We'll create a class that inherits from it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV-fFcT-ZdxB"
      },
      "outputs": [],
      "source": [
        "from transformers import Tool\n",
        "\n",
        "class CatImageFetcher(Tool):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYGJHeSjHDgV"
      },
      "source": [
        "This class has a few needs:\n",
        "\n",
        "- An attribute name, which corresponds to the name of the tool itself. To be in tune with other tools which have a performative name, we'll name it text-download-counter.\n",
        "- An attribute description, which will be used to populate the prompt of the agent.\n",
        "- inputs and outputs attributes. Defining this will help the python interpreter make educated choices about types, and will allow for a gradio-demo to be spawned when we push our tool to the Hub. They're both a list of expected values, which can be text, image, or audio.\n",
        "- A __call__ method which contains the inference code. This is the code we've played with above!\n",
        "\n",
        "Hereâ€™s what our class looks like now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05xxmFZ9i4MM"
      },
      "outputs": [],
      "source": [
        "from transformers import Tool\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "\n",
        "class CatImageFetcher(Tool):\n",
        "    name = \"cat_fetcher\"\n",
        "    description = (\"This is a tool that fetches an actual image of a cat online. It takes no input, and returns the image of a cat.\")\n",
        "\n",
        "    inputs = []\n",
        "    outputs = [\"text\"]\n",
        "\n",
        "    def __call__(self):\n",
        "        return Image.open(requests.get('https://cataas.com/cat', stream=True).raw).resize((256, 256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9de6sn8HjDj"
      },
      "source": [
        "We can simply use and test the tool directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNFuJ0Dgi9_V"
      },
      "outputs": [],
      "source": [
        "tool = CatImageFetcher()\n",
        "tool()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgheEUKqJ6RT"
      },
      "source": [
        "In order to pass the tool to the agent, we recommend instantiating the agent with the tools directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2FbsfD2HtU8"
      },
      "outputs": [],
      "source": [
        "from transformers.tools import HfAgent\n",
        "\n",
        "agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\", additional_tools=[tool])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mphUyTyDKHT1"
      },
      "source": [
        "Let's try to have the agent use it with other tools!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6dWvZvtKFBT"
      },
      "outputs": [],
      "source": [
        "agent.run(\"Fetch an image of a cat online and caption it for me\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdMo9h_QKxID"
      },
      "source": [
        "Success ğŸ‰\n",
        "\n",
        "The tool was used to fetch a cat image, and the image captioning tool was used shortly after in order to caption that same image.\n",
        "\n",
        "Finally, we recommend pushing the tool to the Hub in order to have others benefit from it. Here is the documentation that contains more information in order to do so: [Adding a new tool](https://huggingface.co/docs/transformers/en/custom_tools#creating-a-new-tool)\n",
        "\n",
        "Thanks for following through with the notebook! We're looking forward to the tools you'll push, which will help empower all agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahp5TROjkChx"
      },
      "outputs": [],
      "source": [
        "agent.run(\n",
        "    \"Create a dataset (DO NOT try to download one, you MUST create one based on what you find) on the performance of the Mercedes AMG F1 team in 2020 and do some analysis. You need to plot your results. The data can be found in https://en.wikipedia.org/wiki/Mercedes-Benz_in_Formula_One\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DGMTvap2dkac"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}