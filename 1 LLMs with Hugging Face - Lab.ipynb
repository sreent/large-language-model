{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYpUGoqn8qzb52dUDedsMJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sreent/large-language-model/blob/main/1%20LLMs%20with%20Hugging%20Face%20-%20Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs with Hugging Face\n",
        "\n",
        "**Choosing a pre-trained LLM**: In the demo notebook, you saw how to apply pre-trained models to many applications.  You will do that hands-on in this lab, with your main activity being to find a good model for each task.  Use the tips from the lecture and demo to find good models, and don't hesitate to try a few different possibilities.\n",
        "\n",
        "**Understanding LLM pipeline configurations**: At the end of this lab, you will also do a more open-ended exploration of model and tokenizer configurations.\n",
        "\n",
        "### Learning Objectives\n",
        "1. Practice finding an existing model for tasks you want to solve with LLMs.\n",
        "2. Understand the basics of model and tokenizer options for tweaking model outputs and performance.\n"
      ],
      "metadata": {
        "id": "temzopyroY0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries:\n",
        "* [transformers](https://github.com/huggingface/transformers) is for Hugging Face's transformer models\n",
        "* [datasets](https://github.com/huggingface/datasets) is for Hugging Face's datasets\n",
        "* [sacremoses](https://github.com/alvations/sacremoses) is for the translation model `Helsinki-NLP/opus-mt-en-es`\n",
        "* [sentencepiece](https://github.com/google/sentencepiece) is SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece."
      ],
      "metadata": {
        "id": "26Fz1ph1oot0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers\n",
        "!pip -q install git+https://github.com/huggingface/datasets\n",
        "!pip -q install sacremoses==0.0.53\n",
        "!pip -q install sentencepiece\n",
        "!pip -q install accelerate>=0.12.0"
      ],
      "metadata": {
        "id": "vZ3TlX_vocfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find good models for your tasks\n",
        "\n",
        "In each subsection below, you will solve a given task with an LLM of your choosing.  These tasks vary from straightforward to open-ended:\n",
        "* **Summarization**: There are many summarization models out there, and many are simply plug-and-play.\n",
        "* **Translation**: This task can require more work since models support varying numbers of languages, and in different ways.  Make sure you invoke your chosen model with the right parameters.\n",
        "* **Few-shot learning**: This task is very open-ended, where you hope to demonstrate your goals to the LLM with just a few examples.  Choosing those examples and phrasing your task correctly can be more art than science.\n",
        "\n",
        "Recall these tips from the lecture and demo:\n",
        "* Use the [Hugging Face Hub](https://huggingface.co/models).\n",
        "* Filter by task, license, language, etc. as needed.\n",
        "* If you have limited compute resources, check model sizes to keep execution times lower.\n",
        "* Search for existing examples as well.  It can be helpful to see exactly how models should be loaded and used.\n"
      ],
      "metadata": {
        "id": "AQsVdDpIouxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "21WjBT_Koxix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: Summarization\n",
        "\n",
        "In this section, you will find a model from the Hugging Face Hub for a new summarization problem. **Do not use a T5 model**; find and use a model different from the one we used in the demo notebook.\n",
        "\n",
        "We will use the same [xsum](https://huggingface.co/datasets/xsum) dataset.\n"
      ],
      "metadata": {
        "id": "mawjy_DGo0KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_dataset = load_dataset(\n",
        "    \"xsum\", version=\"1.2.0\", cache_dir=DA.paths.datasets\n",
        ")  # Note: We specify cache_dir to use predownloaded data.\n",
        "xsum_sample = xsum_dataset[\"train\"].select(range(10))\n",
        "display(xsum_sample.to_pandas())"
      ],
      "metadata": {
        "id": "oLaGPIiuo3SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to how we found and applied a model for summarization previously, fill in the missing parts below to create a pipeline using an existing LLM---but with a different model.  Then apply the pipeline to the sample batch of articles.\n"
      ],
      "metadata": {
        "id": "1fSQF9AGo6YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "# Constructor a summarization pipeline\n",
        "summarizer = <FILL_IN>\n",
        "\n",
        "# Apply the pipeline to the batch of articles in `xsum_sample`\n",
        "summarization_results = <FILL_IN>\n",
        "summarization_results\n"
      ],
      "metadata": {
        "id": "D0B61XSyo8tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the generated summary side-by-side with the reference summary and original document.\n",
        "import pandas as pd\n",
        "\n",
        "display(\n",
        "    pd.DataFrame.from_dict(summarization_results)\n",
        "    .rename({\"summary_text\": \"generated_summary\"}, axis=1)\n",
        "    .join(pd.DataFrame.from_dict(xsum_sample))[\n",
        "        [\"generated_summary\", \"summary\", \"document\"]\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "yxBze9tapC5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: Translation\n",
        "\n",
        "In this section, you will find a model from the Hugging Face Hub for a new translation problem.\n",
        "\n",
        "We will use the [Helsinki-NLP/tatoeba_mt](https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt) dataset.  It includes sentence pairs from many languages, but we will focus on translating Japanese to English.\n",
        "\n",
        "Hints in case you feel stuck on this task:\n",
        "* Some models can handle *a lot* of languages.  Check out [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb), the No Language Left Behind model ([research paper](https://arxiv.org/abs/2207.04672)).\n",
        "* The \"translation\" task for `pipeline` takes optional parameters `src_lang` (source language) and `tgt_lang` (target language), which are important when the model can handle multiple languages.  To figure out what codes to use to specify languages (and scripts for those languages), it can be helpful to find existing examples of using your model; for NLLB, check out [this Python script with codes](https://huggingface.co/spaces/Geonmo/nllb-translation-demo/blob/main/flores200_codes.py) or similar demo resources."
      ],
      "metadata": {
        "id": "HLa_7XfFpFv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jpn_dataset = load_dataset(\n",
        "    \"Helsinki-NLP/tatoeba_mt\",\n",
        "    version=\"1.0.0\",\n",
        "    language_pair=\"eng-jpn\",\n",
        "    cache_dir=DA.paths.datasets,\n",
        ")\n",
        "jpn_sample = (\n",
        "    jpn_dataset[\"test\"]\n",
        "    .select(range(10))\n",
        "    .rename_column(\"sourceString\", \"English\")\n",
        "    .rename_column(\"targetString\", \"Japanese\")\n",
        "    .remove_columns([\"sourceLang\", \"targetlang\"])\n",
        ")\n",
        "display(jpn_sample.to_pandas())"
      ],
      "metadata": {
        "id": "C8hV6OuApIZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAGIC %md\n",
        "# MAGIC\n",
        "# MAGIC Similarly to how we previously found and applied a model for translation among other languages, you must now find a model to translate from Japanese to English.  Fill in the missing parts below to create a pipeline using an existing LLM.  Then apply the pipeline to the sample batch of Japanese sentences.\n"
      ],
      "metadata": {
        "id": "5LY2GnSkpMni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "# Construct a pipeline for translating Japanese to English.\n",
        "translation_pipeline = <FILL_IN>\n",
        "\n",
        "# Apply your pipeline on the sample of Japanese text in: jpn_sample[\"Japanese\"]\n",
        "translation_results = translation_pipeline(jpn_sample[\"Japanese\"])\n"
      ],
      "metadata": {
        "id": "yVWCZKpepPCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can display your translations side-by-side with the ground-truth `English` column from the dataset.\n",
        "translation_results_df = pd.DataFrame.from_dict(translation_results).join(\n",
        "    jpn_sample.to_pandas()\n",
        ")\n",
        "display(translation_results_df)\n"
      ],
      "metadata": {
        "id": "u58OciD7pSkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3: Few-shot learning\n",
        "\n",
        "In this section, you will build a prompt which gets an LLM to answer a few-shot learning problem.  Your prompt will have 3 sections:\n",
        "\n",
        "1. High-level instruction about the task\n",
        "2. Examples of query-answer pairs for the LLM to learn from\n",
        "3. New query\n",
        "\n",
        "Your goal is to make the LLM answer the new query, with as good a response as possible.\n",
        "\n",
        "More specifically, your prompt should follow this template:\n",
        "```\n",
        "<High-level instruction about the task: Given input_label, generate output_label.>:\n",
        "\n",
        "[<input_label>]: \"<input text>\"\n",
        "[<output_label>]: \"<output_text>\"\n",
        "###\n",
        "[<input_label>]: \"<input text>\"\n",
        "[<output_label>]: \"<output_text>\"\n",
        "###\n",
        "[<input_label>]: \"<input text>\"\n",
        "[<output_label>]:\n",
        "```\n",
        "where the final two lines represent the new query.\n",
        "\n",
        "It is up to you to choose a task, but here are some ideas:\n",
        "* Translation: This is easy but less interesting since there are already models fine-tuned for translation.  You can generate examples via a tool like Google Translate.\n",
        "* Create book titles or descriptions: Given a book title, generate a description, or vice versa.  You can get examples off of Wikipedia.\n",
        "* Generate tweets: Given keywords or a key message, generate a tweet.\n",
        "* Identify the subject: Given a sentence, extract the noun or name of the subject of the sentence.\n",
        "\n",
        "Tips:\n",
        "* If the model gives bad outputs with only 1 or 2 examples, try adding more.  3 or 4 examples can be much better than 1 or 2.\n",
        "* Not all tasks are equally difficult.  If your task is too challenging, try a different one.\n"
      ],
      "metadata": {
        "id": "BKAkjGsjrPIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_pipeline = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=\"EleutherAI/gpt-neo-1.3B\",\n",
        "    max_new_tokens=50,\n",
        "    model_kwargs={\"cache_dir\": DA.paths.datasets},\n",
        ")  # Use a predownloaded model\n",
        "\n",
        "# Get the token ID for \"###\", which we will use as the EOS token below.  (Recall we did this in the demo notebook.)\n",
        "eos_token_id = few_shot_pipeline.tokenizer.encode(\"###\")[0]\n"
      ],
      "metadata": {
        "id": "Pmg_QWlTpY9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the template below.  Feel free to adjust the number of examples.\n"
      ],
      "metadata": {
        "id": "yPBQkM8ipc0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "# Fill in this template.\n",
        "\n",
        "prompt =\\\n",
        "\"\"\"<High-level instruction about the task>:\n",
        "\n",
        "[<input_label>]: \"<input text>\"\n",
        "[<output_label>]: \"<output_text>\"\n",
        "###\n",
        "[<input_label>]: \"<input text>\"\n",
        "[<output_label>]: \"<output_text>\"\n",
        "###\n",
        "[<input_label>]: \"<input text>\"\n",
        "[<output_label>]:\"\"\"\n"
      ],
      "metadata": {
        "id": "iI_T8kMMpey3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = few_shot_pipeline(prompt, do_sample=True, eos_token_id=eos_token_id)\n",
        "\n",
        "print(results[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "4I3ncPxnph-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore model and tokenizer settings\n",
        "\n",
        "So far, we have used pipelines in a very basic way, without worrying about configuration options.  In this section, you will explore the various options for models and tokenizers to learn how they affect LLM behavior.\n",
        "\n",
        "We will load a dataset, tokenizer, and model for you.  We will also define a helper method for printing out results nicely.\n"
      ],
      "metadata": {
        "id": "tpSfpleApmbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data, tokenizer, and model.\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "xsum_dataset = load_dataset(\"xsum\", version=\"1.2.0\", cache_dir=DA.paths.datasets)\n",
        "xsum_sample = xsum_dataset[\"train\"].select(range(10))\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", cache_dir=DA.paths.datasets)\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"t5-small\", cache_dir=DA.paths.datasets\n",
        ")\n",
        "\n",
        "# Prepare articles for T5, which requires a \"summarize: \" prefix.\n",
        "articles = list(map(lambda article: \"summarize: \" + article, xsum_sample[\"document\"]))\n"
      ],
      "metadata": {
        "id": "KzsSrK13pot6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_summaries(decoded_summaries: list) -> None:\n",
        "    \"\"\"Helper method to display ground-truth and generated summaries side-by-side\"\"\"\n",
        "    results_df = pd.DataFrame(zip(xsum_sample[\"summary\"], decoded_summaries))\n",
        "    results_df.columns = [\"Summary\", \"Generated\"]\n",
        "    display(results_df)"
      ],
      "metadata": {
        "id": "RNojEzbTprvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open-ended exploration\n",
        "\n",
        "In the cells below, we provide code for running the tokenizer and model on the articles.  Your task is to play around with the various configurations to gain more intuition about the effects.  Look for changes to output quality and running time in particular, and remember that running the same code twice may result in different answers.\n",
        "\n",
        "Below, we list brief descriptions of each of the parameters you may wish to tweak.\n",
        "* Tokenizer encoding\n",
        "  * `max_length`: This caps the maximum input length.  It must be at or below the model's input length limit.\n",
        "  * `return_tensors`: Do not change this one.  This tells Hugging Face to return tensors in PyTorch (\"pt\") format.\n",
        "* Model\n",
        "  * `do_sample`: True or False.  This tells the model whether or not to use sampling in generation.  If False, then it will do greedy search or beam search.  If True, then it will do random sampling which can optionally use the top-p and/or top-k sampling techniques.  See the blog post linked below for more details on sampling techniques.\n",
        "  * `num_beams`: (for beam search) This specifies the number of beams to use in beam search across possible sequences.  Increasing the number can help the model to find better sequences, at the cost of more computation.\n",
        "  * `min_length`, `max_length`: Generative models can be instructed to generate new text between these token lengths.\n",
        "  * `top_k`: (for sampling) This controls the use of top-K sampling, which forces sampling to ignore low-probability tokens by limiting to the K most probable next tokens.  Set to 0 to disable top-K sampling.\n",
        "  * `top_p`: (for sampling) This controls the use of top-p sampling, which forces sampling to ignore low-probability tokens by limiting to the top tokens making up probability mass p.  Set to 0 to disable top-p sampling.\n",
        "  * `temperature`: (for sampling) This controls the \"temperature\" of the softmax.  Lower values bias further towards high-probability next tokens.  Setting to 0 makes sampling equivalent to greedy search.\n",
        "* Tokenizer decoding\n",
        "  * `skip_special_tokens`: True or False.  This allows you to skip special tokens (like EOS tokens) in the model outputs.\n",
        "\n",
        "Do not tweak:\n",
        "* Tokenizer encoding\n",
        "  * `padding`: True or False.  This helps to handle variable-length inputs by adding padding to short inputs.  Since it should be set according to your task and data, you should not change it for this exercise (unless you want to see what warnings or error may appear).\n",
        "  * `truncation`: True or False.  This helps to handle variable-length inputs by truncating very long inputs.  Since it should be set according to your task and data, you should not change it for this exercise (unless you want to see what warnings or error may appear).\n",
        "\n",
        "If you need more info about the parameters of methods, see the `help()` calls in cells below, or search the Hugging Face docs.  Some top links are:\n",
        "* Tokenizer call for encoding: [PreTrainedTokenizerBase.\\_\\_call\\_\\_ API docs](https://huggingface.co/docs/transformers/v4.28.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)\n",
        "* Model invocation: [Docs for generation strategies](https://huggingface.co/docs/transformers/main/en/generation_strategies) and this blog post on [\"How to generate text: using different decoding methods for language generation with Transformers\"](https://huggingface.co/blog/how-to-generate)\n",
        "\n",
        "If you mess up and can't get back to a working state, you can use the Revision History to revert your changes.\n",
        "Access that via the clock-like icon or \"Revision History\" button in the top-right of this notebook page. (See screenshot below.)\n",
        "\n",
        "![Screenshot of notebook Revision History](https://files.training.databricks.com/images/llm/revision_history.png)\n"
      ],
      "metadata": {
        "id": "ROLXNAarpued"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "# TODO: Try editing the parameters in this section, and see how they affect the results.\n",
        "#       You can also copy and edit the cell to compare results across different parameter settings.\n",
        "#\n",
        "# We show all parameter settings for ease-of-modification, but in practice, you would only set relevant ones.\n",
        "inputs = tokenizer(\n",
        "    articles, max_length=1024, return_tensors=\"pt\", padding=True, truncation=True\n",
        ")\n",
        "\n",
        "summary_ids = model.generate(\n",
        "    inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    do_sample=True,\n",
        "    num_beams=2,\n",
        "    min_length=0,\n",
        "    max_length=40,\n",
        "    top_k=20,\n",
        "    top_p=0.5,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "decoded_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
        "##############################################################################\n",
        "\n",
        "display_summaries(decoded_summaries)"
      ],
      "metadata": {
        "id": "LMAJrPgfpys7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment `help()` calls below as needed to see doc strings for stages of the pipeline.\n"
      ],
      "metadata": {
        "id": "FoJuDkA6p3f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Options for calling the tokenizer (lots to see here)\n",
        "# help(tokenizer.__call__)"
      ],
      "metadata": {
        "id": "4b3RDttbp6--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Options for invoking the model (lots to see here)\n",
        "# help(model.generate)\n"
      ],
      "metadata": {
        "id": "DV8Z0fLap-Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Options for calling the tokenizer for decoding (not much to see here)\n",
        "# help(tokenizer.batch_decode)"
      ],
      "metadata": {
        "id": "T489jkAeqBaE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}